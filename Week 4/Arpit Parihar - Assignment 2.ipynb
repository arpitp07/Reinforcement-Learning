{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Reinforcement Learning - Assignment 2\n"," ## Arpit Parihar\n"," ## 05/04/2021\n"," ****"]},{"cell_type":"markdown","metadata":{},"source":[" A child has available a certain number of ice cream scoops every day, $s$. The child can store a number of scoops for the next day $a$ and eat the remainder scoops $c = s − a$. Mathematically,\n"," $$s = a + c$$\n"," where $a \\in \\{0, 1, .., s\\}$. The number of scoops available the next day, $s'$, is equal to the number of scoops stored over night, $a$, plus an additional scoops provided by the parents, $e$. The number of scoops available the next day is given by:\n","\n"," $$\n"," \\begin{eqnarray}\n"," s'= && a + e' \\\\\n","   = && (s - c) + e'\n"," \\end{eqnarray}\n"," $$\n","\n"," where $e' \\in \\{0, 1, 2\\}$. $e'$ is known after action $a$ is taken. The child can store up to $2$ scoops in the fridge every day which implies that $a \\in \\{0, 1, 2\\}$ and $s \\in \\{0, 1, 2, 3, 4\\}$. The transition probability matrix from $e$ to $e'$ is given by:\n","\n"," $$P = \\begin{bmatrix} 0.8 & 0.1 & 0.1 \\\\ 0.01 & 0.98 & 0.01 \\\\ 0.1 & 0.1 & 0.8 \\end{bmatrix}$$"]},{"cell_type":"markdown","metadata":{},"source":[" Importing modules"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from IPython.display import display\n","import matplotlib.pyplot as plt\n","import joblib\n","import math\n","import gym\n","from gym import spaces\n","import random\n","from matplotlib.ticker import FormatStrFormatter\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":[" - **Construct the transition probability from $(s,e)$ to $(s′,e′)$ for each action $a \\in \\{0,1,2\\}$.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s = range(5)\n","e = range(3)\n","a = range(3)\n","\n","p_e = np.array([\n","    [0.8, 0.1, 0.1],\n","    [0.01, 0.98, 0.01],\n","    [0.1, 0.1, 0.8]],\n","    dtype=float\n",")\n","\n","P = np.zeros((len(a), len(s) * len(e), len(s) * len(e)), dtype=np.float64)\n","R = np.zeros((len(a), len(s) * len(e), len(s) * len(e)), dtype=np.float64)\n","\n","for i in a:\n","    for j in range(len(s) * len(e)):\n","        P[i, j, i + np.arange(len(e)) * (len(s) + 1)] = p_e[j // len(s)]\n","        R[i, j, :] = math.log(\n","            j % len(s) - i + 1) if j % len(s) - i >= 0 else -math.inf\n","\n","states = [f'({y}, {x})' for x in e for y in s]\n","P_mat = [pd.DataFrame(P[x, :, :], columns=states, index=states) for x in a]\n","for x in range(len(P_mat)):\n","    print(f'Transition matrix for a = {x}:\\n')\n","    P_mat[x]\n","    print('\\n')"]},{"cell_type":"markdown","metadata":{},"source":[" - **The child subjected utility from eating ice cream is $log(c+1)$ if $c≥0$ and $−\\infty$ otherwise. Construct the reward matrix for each transition $(s,e,a)$ and $(s′,e′)$.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["R_mat = [pd.DataFrame(R[x, :, :], columns=states, index=states) for x in a]\n","for x in range(len(R_mat)):\n","    print(f'Reward matrix for a = {x}:\\n')\n","    R_mat[x]\n","    print('\\n')"]},{"cell_type":"markdown","metadata":{},"source":[" - **Apply the value iteration approach to compute the value function for each state $(s,e)$ and optimal policy. Iterate $600$ times.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating q vector\n","q = np.zeros((len(a), len(s) * len(e)), dtype=np.float64)\n","\n","for i in a:\n","    for j in range(len(s) * len(e)):\n","        q[i, j] = np.nan_to_num(np.dot(P[i, j, :], R[i, j, :]), nan=-math.inf)\n","\n","T = 600\n","v = np.zeros((len(states), T + 1),dtype=np.float64)\n","d = np.zeros((len(states), T + 1),dtype=np.float64)\n","\n","for t in range(1, T + 1):\n","    for i in range(len(states)):\n","        rhs = np.zeros(len(a), dtype=np.float64)\n","        for j in a:\n","            rhs[j] = q[j, i] + np.matmul(P[j, i, :], v[:, t - 1])\n","        v[i, t] = max(rhs)\n","        d[i, t] = np.argmax(rhs)\n","\n","v = pd.DataFrame(v.T, columns=states)\n","d = pd.DataFrame(d.T, columns=states)"]},{"cell_type":"markdown","metadata":{},"source":[" - **For each $e$, plot the value function $v(s,e)$ with $s$ on the x-axis. (hint: there should be 3 lines plots, one for each $e\\in\\{0,1,2\\}$).**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_data = np.zeros((len(s), len(e)))\n","for i in range(len(states)):\n","    row = eval(v.iloc[-1].index[i])[0]\n","    column = eval(v.iloc[-1].index[i])[1]\n","    plot_data[row, column] = v.iloc[-1, i]\n","\n","plt.plot(plot_data)\n","plt.xlabel('s')\n","plt.xticks(ticks=s)\n","plt.ylabel('Expected Value')\n","plt.legend(e)\n","plt.title('Average Value for each s and e')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":[" **For each $e$, plot the optimal policy for storing ice cream scoops $a(s,e)$ with $s$ on the x-axis. (hint: there should be 3 lines plots, one for each $e\\in\\{0,1,2\\}$).**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_data = np.zeros((len(s), len(e)))\n","for i in range(len(states)):\n","    row = eval(d.iloc[-1].index[i])[0]\n","    column = eval(d.iloc[-1].index[i])[1]\n","    plot_data[row, column] = d.iloc[-1, i]\n","\n","plt.plot(plot_data)\n","plt.xlabel('s')\n","plt.xticks(ticks=s)\n","plt.ylabel('Optimal Policy - Scoops Saved')\n","plt.yticks(a)\n","plt.legend(e)\n","plt.title('Optimal Policy for each s and e')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":[" - **For each $e$, plot the optimal policy for consuming ice cream scoops $c(s,e)$ with $s$ on the x-axis. (hint: there should be 3 lines plots, one for each $e\\in\\{0,1,2\\}$).**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_data = np.zeros((len(s), len(e)))\n","for i in range(len(states)):\n","    row = eval(d.iloc[-1].index[i])[0]\n","    column = eval(d.iloc[-1].index[i])[1]\n","    plot_data[row, column] = row - d.iloc[-1, i]\n","\n","plt.plot(plot_data)\n","plt.xlabel('s')\n","plt.xticks(ticks=s)\n","plt.ylabel('Optimal Policy - Scoops Eaten')\n","plt.yticks(a)\n","plt.legend(e)\n","plt.title('Optimal Policy for each s and e')\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":[" - **Simulate a sequence of $e$ and set an initial value for $s$. Given the optimal policy, calculate and plot the evolution of $a, c,$ and $s$ over time.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[" - **Construct the transition probability and reward matrices between $(s,e)$ and $(s′,e′)$ that produces the highest expected discount rewards. (hint: you need to use the optimal policy)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[" - **Calculate the value function for the Markok process with rewards that produces the highest expected discount rewards. (hint: use the transition probability and reward matrices from the previous question). Does the value function matches reasonably the value function from the previous question? (hint: it should)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[" - **Simulate the Markov process with rewards from the previous question for starting at each state pair $(e,s)$. Compute the average discounted reward. Does it match reasonabily close to the value function in the previous question? (hint: it should)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[" - **Calculate the optimal policy based on the policy iteration approach.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 64-bit ('base': conda)","name":"python385jvsc74a57bd00d32af75ebd2bb64dbadaa35163c36ed836708b81c767a94a67197c5be7abd40"},"language_info":{"name":"python","version":""},"orig_nbformat":2},"nbformat":4,"nbformat_minor":2}