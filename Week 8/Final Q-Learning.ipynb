{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Reinforcement Learning\n"," ## Final Project - The Lazy Gardener\n"," ### Contributors:\n"," - Nina Randorf\n"," - Remy Zhong\n"," - Arpit Parihar"]},{"cell_type":"markdown","metadata":{},"source":[" ### Importing modules"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from TLG_env import garden_env"]},{"cell_type":"markdown","metadata":{},"source":[" ### Declare environment constants and transition matrices"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dictionaries to keep track of weather, bunny and actions\n","WEATHER = {0: 'Sunny', 1: 'Rainy', 2: 'Stormy'}\n","BUNNY = {0: 'No_Bunny', 1: 'Bunny'}\n","ACTIONS = {-1: 'Pump', 0: 'Nothing', 1: 'Water'}\n","\n","# List of possible saturation states\n","STATES = list(range(-1, 6))\n","\n","# Weather to weather transition matrix\n","P_WEATHER = pd.DataFrame(\n","    {\n","        0: [0.75, 0.3, 0.4],\n","        1: [0.2, 0.5, 0.4],\n","        2: [0.05, 0.2, 0.2]\n","    })\n","\n","# Weather to bunny transition matrix\n","P_BUNNY = pd.DataFrame(\n","    {\n","        0: [0.2, 0.8],\n","        1: [0.6, 0.4],\n","        2: [0.9, 0.1]\n","    })"]},{"cell_type":"markdown","metadata":{},"source":[" ## Creating environment instance for Q-Learning algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["QL_Agent = garden_env(WEATHER, BUNNY, ACTIONS, STATES, P_WEATHER, P_BUNNY)"]},{"cell_type":"markdown","metadata":{},"source":[" ### Declaring algorithm constants"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["LEARNING_RATE = 0.1\n","DISCOUNT = 0.99\n","EPISODES = 10000\n","DAYS_TILL_HARVEST = 50\n","HARVEST_REWARD = 0\n","\n","epsilon = 1\n","START_EPSILON_DECAYING = 1\n","END_EPSILON_DECAYING = EPISODES // 2\n","epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Algorithm loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","q_table = np.random.uniform(low=-0.5, high=0.5, size=(len(QL_Agent.P_ind), len(QL_Agent.A)))\n","\n","for episode in range(EPISODES):\n","    QL_Agent.reset_env()\n","    state = QL_Agent.state\n","    for day in range(DAYS_TILL_HARVEST):\n","        if np.random.random() > epsilon:\n","            action = np.argmax(q_table[QL_Agent.P_ind.index(state)])\n","        else:\n","            action = np.random.randint(0, len(QL_Agent.A))\n","        \n","        # rand_num = np.random.random()\n","        # P_cur = pd.DataFrame(P[action], index=P_ind, columns=P_ind)\n","        # state_new = pd.Series(P_ind)[P_cur.loc[state, :].cumsum().ge(rand_num).tolist()].iloc[0]\n","        QL_Agent.step(action)\n","        state_new = QL_Agent.state\n","        \n","        # reward = R[action, P_ind.index(state), P_ind.index(state_new)]\n","        reward = QL_Agent.reward if day < DAYS_TILL_HARVEST - 1 else QL_Agent.reward + HARVEST_REWARD\n","        \n","        max_future_q = np.max(q_table[QL_Agent.P_ind.index(state_new)])\n","        current_q = q_table[QL_Agent.P_ind.index(state), action]\n","        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n","        \n","        q_table[QL_Agent.P_ind.index(state), action] = new_q\n","        state = state_new\n","        if eval(state)[0] in [min(QL_Agent.S), max(QL_Agent.S)]:\n","            break\n","    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n","        epsilon -= epsilon_decay_value"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["policy = pd.Series([QL_Agent.A[x] for x in q_table.argmax(axis=1) - 1], index=QL_Agent.P_ind)\n","# policy = pd.Series([x for x in q_table.argmax(axis=1) - 1], index=QL_Agent.P_ind)\n","Q_table_final = pd.DataFrame(q_table, columns=(QL_Agent.A.values()), index=QL_Agent.P_ind)\n","Q_table_final['Policy_QLearning'] = policy\n","# Q_table_final\n","# # %%\n","# # %%\n","# # %%\n","# QL_Agent.step(-1)\n","# QL_Agent.state\n","# QL_Agent.reward\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating q vector\n","q = np.zeros((len(ACTIONS), len(QL_Agent.P_ind)), dtype=np.float64)\n","\n","for i in ACTIONS:\n","    for j in range(len(QL_Agent.P_ind)):\n","        q[i, j] = np.dot(QL_Agent.P[i, j, :], QL_Agent.R[i, :])\n","\n","T = 600\n","v = np.zeros((len(QL_Agent.P_ind), T + 1),dtype=np.float64)\n","d = np.zeros((len(QL_Agent.P_ind), T + 1),dtype=np.float64)\n","\n","for t in range(1, T + 1):\n","    for i in range(len(QL_Agent.P_ind)):\n","        rhs = np.zeros(len(ACTIONS), dtype=np.float64)\n","        for j in ACTIONS:\n","            rhs[j] = q[j, i] + DISCOUNT*np.matmul(QL_Agent.P[j, i, :], v[:, t - 1])\n","        v[i, t] = max(rhs)\n","        d[i, t] = np.argmax(rhs)\n","\n","v = pd.DataFrame(v.T, columns=QL_Agent.P_ind)\n","d = pd.DataFrame(d.T, columns=QL_Agent.P_ind)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Q_table_final['Policy_QLearning'] = policy\n","policy = pd.Series([QL_Agent.A[x] for x in d.iloc[len(v)-1, :] - 1], index=QL_Agent.P_ind)\n","Q_table_final['Policy_VIter'] = policy\n","Q_table_final.loc[[eval(x)[0] not in [min(QL_Agent.S), max(QL_Agent.S)] for x in QL_Agent.P_ind], :]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","v = np.zeros((len(QL_Agent.P_ind), T + 1),dtype=np.float64)\n","d = np.zeros((len(QL_Agent.P_ind), T + 1),dtype=np.float64)\n","\n","for t in range(1, T + 1):\n","    for i in range(len(QL_Agent.P_ind)):\n","        rhs = np.zeros(len(ACTIONS), dtype=np.float64)\n","        for j in ACTIONS:\n","            rhs[j] = q[j, i] + DISCOUNT*np.matmul(QL_Agent.P[j, i, :], v[:, t - 1])\n","        v[i, t] = max(rhs)\n","        d[i, t] = np.argmax(rhs)\n","\n","    PP = np.array([QL_Agent.P[j, i, :] for (i, j) in enumerate(d[:, t].astype('int'))])\n","    A = np.concatenate((np.identity(len(QL_Agent.P_ind)) - PP, np.ones((len(QL_Agent.P_ind), 1))), axis=1)\n","    A = np.identity(len(QL_Agent.P_ind)) - PP\n","    A = np.delete(A, len(QL_Agent.P_ind)-1, 1)\n","    \n","    qq = np.array([q[j, i] for (i, j) in enumerate(d[:, t].astype('int'))])\n","    tmp = np.matmul(np.linalg.inv(A), qq)\n","    g = tmp[len(QL_Agent.P_ind)-1]\n","    tmp[len(QL_Agent.P_ind)-1] = 0\n","    v[:, t] = tmp.T\n","    \n","v = pd.DataFrame(v.T, columns=QL_Agent.P_ind)\n","d = pd.DataFrame(d.T, columns=QL_Agent.P_ind)\n","d.iloc[-1]"]},{"cell_type":"markdown","metadata":{},"source":["### DQN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}